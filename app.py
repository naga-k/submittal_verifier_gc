import re
import streamlit as st
from pypdf import PdfReader
from openai import OpenAI
import json
import time
import concurrent.futures
import os

# ---------------------------
#  CONFIG
# ---------------------------
st.set_page_config(page_title="GC Submittal Completeness Checker", layout="wide")
API_KEY = st.secrets.get("OPENAI_API_KEY")
if not API_KEY:
    st.error('Please add your OpenAI API key to .streamlit/secrets.toml as OPENAI_API_KEY')
    st.stop()
client = OpenAI(api_key=API_KEY)


# ---------------------------
#  UTILS
# ---------------------------
def extract_text_from_pdf(uploaded_file) -> str:
    reader = PdfReader(uploaded_file)
    return "\n".join([page.extract_text() or "" for page in reader.pages])


# NEW: chunked extraction + formatting + optional logging
def log_cleaned_pdf_text(text: str, filename: str) -> str | None:
    """
    Writes cleaned text to ./temp/{filename}.
    Returns filepath on success, None on failure.
    """
    try:
        os.makedirs("temp", exist_ok=True)
        filepath = os.path.join("temp", filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(text)
        return filepath
    except Exception:
        return None


def extract_and_format_pdf(uploaded_file, chunk_size: int = 5, enable_logging: bool = False, label: str = "document") -> str:
    """
    Extracts PDF text page-by-page, chunks into `chunk_size` pages,
    sends each chunk to the LLM formatter (ask_llm) and preserves page markers.

    Returns the concatenated cleaned/formatted text (with page markers).
    Optionally writes cleaned text to cleaned_{label}.txt when enable_logging=True.
    """
    reader = PdfReader(uploaded_file)
    pages = [page.extract_text() or "" for page in reader.pages]
    if not pages:
        return ""

    # sanitize chunk_size bounds (1..10)
    chunk_size = max(1, min(int(chunk_size), 10))
    cleaned_chunks = []

    for start in range(0, len(pages), chunk_size):
        chunk_pages = pages[start : start + chunk_size]

        # Build page-marked text for this chunk. Preserve markers exactly.
        formatted_input = ""
        for i, raw_text in enumerate(chunk_pages):
            page_number = start + i + 1
            formatted_input += f"--- PAGE {page_number} START ---\n{raw_text}\n--- PAGE {page_number} END ---\n\n"

        prompt = f"""
You are a PDF formatting assistant. Clean and format the text below.
Keep all numbers, tables, and units intact.
Always preserve page markers exactly as provided.

{formatted_input}
"""
        try:
            # use gpt-5-nano for PDF formatting per request
            formatted_output = ask_llm(prompt, model="gpt-5-nano") or ""
        except Exception:
            formatted_output = formatted_input  # fallback to raw page-marked text
        cleaned_chunks.append(formatted_output.strip())

    full_cleaned_text = "\n\n".join([c for c in cleaned_chunks if c])

    if enable_logging:
        filename = f"cleaned_{label}.txt"
        saved = log_cleaned_pdf_text(full_cleaned_text, filename)
        if saved:
            # Note: Streamlit UI will show an info message where this function is called.
            pass

    return full_cleaned_text


def ask_llm(prompt: str, model: str = "gpt-5-mini") -> str:
    """
    Uses OpenAI Responses API. Default model gpt-5-mini.
    Accepts optional model override (e.g., 'gpt-5-nano' for formatting).
    Returns best-effort plain text.
    """
    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": "You are a construction workflow assistant and GC project manager."},
            {"role": "user", "content": prompt},
        ],
    )

    # Preferred convenience property
    if hasattr(resp, "output_text") and resp.output_text:
        return resp.output_text

    # Fallback: walk output pieces
    try:
        pieces = []
        for out in getattr(resp, "output", []):
            for item in out.get("content", []):
                if item.get("type") == "output_text":
                    pieces.append(item.get("text", ""))
        return "\n".join(pieces)
    except Exception:
        return ""


def parse_llm_json(text: str):
    """
    Robust JSON extraction from LLM text. Returns dict/list/None.
    """
    if not text:
        return None
    # Try direct JSON
    try:
        return json.loads(text)
    except Exception:
        pass

    # Try to extract first JSON object/array in text
    m = re.search(r"(\{.*\}|\[.*\])", text, flags=re.S)
    if not m:
        return None
    try:
        return json.loads(m.group(0))
    except Exception:
        return None


def normalize_checklist(obj):
    """
    Ensure checklist is dict with 'submittals' list of dicts with id/text.
    """
    if not obj:
        return {"submittals": []}
    if isinstance(obj, dict) and "submittals" in obj and isinstance(obj["submittals"], list):
        subs = obj["submittals"]
    elif isinstance(obj, list) and all(isinstance(x, dict) for x in obj):
        subs = obj
    else:
        return {"submittals": []}

    out = []
    for i, item in enumerate(subs, start=1):
        if not isinstance(item, dict):
            continue
        entry = {"id": item.get("id", f"S.{i}"), "text": item.get("text", "").strip()}
        out.append(entry)
    return {"submittals": out}


# ---------------------------
#  AGENT: SUBMITTAL PACKAGE CLASSIFIER
# ---------------------------
def classify_submittal_package(submittal_text: str, submittal_filename: str):
    prompt = f"""
You are a GC Project Manager. A submittal package has been uploaded.

Filename: "{submittal_filename}"

Your tasks:
1) Determine the type of submittal package (e.g., "Concrete Mix Design", "Fire Alarm Shop Drawings", "Product Data", "Test Reports", etc.).
2) Summarize what this package contains in 1‚Äì2 short sentences.
Return valid JSON only in this exact shape:
{{"package_type":"<detected type>","summary":"<short human summary>"}}

--- SUBMITTAL TEXT START ---
{submittal_text}
--- SUBMITTAL TEXT END ---
"""
    result = ask_llm(prompt)
    parsed = parse_llm_json(result)
    if isinstance(parsed, dict) and parsed.get("package_type"):
        return {"package_type": str(parsed.get("package_type")), "summary": str(parsed.get("summary", ""))}
    return {"package_type": "Unknown", "summary": "Could not determine the type of submittal package."}


# ---------------------------
#  AGENT: SPEC EXTRACTOR
# ---------------------------
def extract_spec_checklist(spec_text: str):
    prompt = f"""
You are an expert construction spec analyst.
Extract ALL submittal requirements from the spec below. Output valid JSON only in this format:
{{"submittals": [ {{ "id": "S.1", "text": "Submit product data for Portland cement." }}, ... ] }}

--- SPEC START ---
{spec_text}
--- SPEC END ---
"""
    result = ask_llm(prompt)
    parsed = parse_llm_json(result)
    checklist = normalize_checklist(parsed)
    return checklist


# ---------------------------
#  AGENT: PACKAGE VERIFIER
# ---------------------------
def verify_submittal(checklist: dict, submittal_text: str, package_type: str):
    """
    Generator: yields parsed finding dicts for each checklist item.
    Finding shape:
      {"req_id": "...", "status": "present|missing|not_applicable|unclear", "evidence": "..."}
    """
    for item in checklist.get("submittals", []):
        req_id = item.get("id")
        req_text = item.get("text", "")
        prompt = f"""
You are a GC PM verifying a submittal package for completeness.

Submittal package type: "{package_type}"
Requirement from spec: "{req_text}"

Rules:
- If the requirement is relevant to this package type and present ‚Üí status="present"
- If the requirement is relevant but missing ‚Üí status="missing"
- If the requirement exists in the spec but does NOT apply to this package ‚Üí status="not_applicable"
- If unsure ‚Üí status="unclear"

Return strictly valid JSON only, exactly one object with keys: req_id, status, evidence.
Example:
{{"req_id":"{req_id}","status":"present","evidence":"short snippet here"}}

--- SUBMITTAL TEXT START ---
{submittal_text}
--- SUBMITTAL TEXT END ---
"""
        result = ask_llm(prompt)
        parsed = parse_llm_json(result)
        # normalize parsed into a single dict finding
        finding = None
        if isinstance(parsed, dict):
            finding = parsed
        elif isinstance(parsed, list) and parsed:
            first = parsed[0]
            if isinstance(first, dict):
                finding = first

        if not isinstance(finding, dict):
            finding = {"req_id": req_id, "status": "unclear", "evidence": ""}

        # Ensure keys exist and req_id preserved
        finding.setdefault("req_id", req_id)
        finding.setdefault("status", "unclear")
        finding.setdefault("evidence", "")

        yield {"req_id": str(finding["req_id"]), "status": str(finding["status"]), "evidence": str(finding["evidence"])}
        time.sleep(0.12)


def verify_submittal_parallel(checklist: dict, submittal_text: str, package_type: str, max_workers: int = 4):
    """
    Parallelized verification of checklist items using a ThreadPoolExecutor.
    Yields normalized finding dicts as each LLM call completes.
    Each result includes 'index' to allow ordered rendering.
    Note: keep max_workers modest (2-6) to avoid rate limits.
    """
    items = checklist.get("submittals", []) or []

    def worker(item, index):
        req_id = item.get("id")
        req_text = item.get("text", "")
        prompt = f"""
You are a GC PM verifying a submittal package for completeness.

Submittal package type: "{package_type}"
Requirement from spec: "{req_text}"

Rules:
- If the requirement is relevant to this package type and present ‚Üí status="present"
- If the requirement is relevant but missing ‚Üí status="missing"
- If the requirement exists in the spec but does NOT apply to this package ‚Üí status="not_applicable"
- If unsure ‚Üí status="unclear"

Return strictly valid JSON only, exactly one object with keys: req_id, status, evidence.
Example:
{{"req_id":"{req_id}","status":"present","evidence":"short snippet here"}}

--- SUBMITTAL TEXT START ---
{submittal_text}
--- SUBMITTAL TEXT END ---
"""
        try:
            result = ask_llm(prompt)
            parsed = parse_llm_json(result)
            finding = None
            if isinstance(parsed, dict):
                finding = parsed
            elif isinstance(parsed, list) and parsed:
                first = parsed[0]
                if isinstance(first, dict):
                    finding = first
            if not isinstance(finding, dict):
                finding = {"req_id": req_id, "status": "unclear", "evidence": ""}

            # ensure keys exist
            finding.setdefault("req_id", req_id)
            finding.setdefault("status", "unclear")
            finding.setdefault("evidence", "")

            return {"index": index, "req_id": str(finding["req_id"]), "status": str(finding["status"]), "evidence": str(finding["evidence"])}
        except Exception:
            return {"index": index, "req_id": req_id, "status": "unclear", "evidence": ""}

    workers = min(max_workers, max(1, len(items)))
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
        futures = [ex.submit(worker, itm, idx) for idx, itm in enumerate(items)]
        for fut in concurrent.futures.as_completed(futures):
            yield fut.result()


# ---------------------------
#  STREAMLIT UI
# ---------------------------
def main():
    st.title("üìÑ GC Submittal Completeness Checker")
    st.write("Upload a spec PDF and a submittal PDF. Click 'Start LLM Analysis' to run the pipeline.")

    col1, col2 = st.columns(2)
    with col1:
        spec_file = st.file_uploader("Upload Spec PDF", type=["pdf"])
    with col2:
        submittal_file = st.file_uploader("Upload Submittal PDF", type=["pdf"])

    # NEW: logging toggle
    enable_logging = st.checkbox("Enable PDF Text Logging (writes cleaned_spec.txt / cleaned_submittal.txt)", value=False)

    # persistent state
    st.session_state.setdefault("uploaded_names", (None, None))
    st.session_state.setdefault("spec_text", None)
    st.session_state.setdefault("submittal_text", None)
    st.session_state.setdefault("classification", None)
    st.session_state.setdefault("checklist", None)
    st.session_state.setdefault("findings", [])
    st.session_state.setdefault("run_analysis", False)
    st.session_state.setdefault("analysis_done", False)

    if spec_file and submittal_file:
        uploaded = (spec_file.name, submittal_file.name)
        if st.session_state.uploaded_names != uploaded:
            st.session_state.uploaded_names = uploaded
            st.session_state.spec_text = None
            st.session_state.submittal_text = None
            st.session_state.classification = None
            st.session_state.checklist = None
            st.session_state.findings = []
            st.session_state.run_analysis = False
            st.session_state.analysis_done = False

        st.success("‚úÖ Files uploaded")
        if not st.session_state.spec_text:
            with st.spinner("Extracting + formatting spec PDF..."):
                cleaned = extract_and_format_pdf(
                    spec_file, chunk_size=5, enable_logging=enable_logging, label="spec"
                )
                st.session_state.spec_text = cleaned
                if enable_logging:
                    st.info(f"üìù Logged cleaned spec text to ./temp/cleaned_spec.txt")

        if not st.session_state.submittal_text:
            with st.spinner("Extracting + formatting submittal PDF..."):
                cleaned = extract_and_format_pdf(
                    submittal_file, chunk_size=5, enable_logging=enable_logging, label="submittal"
                )
                st.session_state.submittal_text = cleaned
                if enable_logging:
                    st.info(f"üìù Logged cleaned submittal text to ./temp/cleaned_submittal.txt")

        st.subheader("Ready to analyze")
        if st.button("Start LLM Analysis"):
            st.session_state.findings = []
            st.session_state.run_analysis = True
            st.session_state.analysis_done = False

        if st.session_state.run_analysis and not st.session_state.analysis_done:
            # Step 1: classify package
            with st.spinner("Classifying submittal package..."):
                st.session_state.classification = classify_submittal_package(
                    st.session_state.submittal_text, submittal_file.name
                )

            st.subheader("üì¶ Submittal Package Summary")
            st.write("**Type:**", st.session_state.classification.get("package_type", "Unknown"))
            st.write("**Summary:**", st.session_state.classification.get("summary", ""))

            # Step 2: extract checklist
            with st.spinner("Extracting checklist from spec..."):
                raw_checklist = extract_spec_checklist(st.session_state.spec_text)
                st.session_state.checklist = normalize_checklist(raw_checklist)
            st.success(f"Found {len(st.session_state.checklist.get('submittals', []))} submittal requirements.")
            st.json(st.session_state.checklist)

            # Step 3: verify each checklist item (parallel)
            st.subheader("Verifying completeness")
            findings_container = st.empty()

            checklist_items = st.session_state.checklist.get("submittals", [])
            findings_by_index = {}  # store results by original checklist order
            total_items = len(checklist_items)
            pkg_type = st.session_state.classification.get("package_type", "Unknown")

            for res in verify_submittal_parallel(st.session_state.checklist, st.session_state.submittal_text, pkg_type, max_workers=4):
                # normalize res into dict with index
                idx = res.get("index") if isinstance(res, dict) else None
                if idx is None:
                    # fallback: append at end (shouldn't happen if parallel returns index)
                    idx = len(findings_by_index)
                norm = {
                    "req_id": str(res.get("req_id", "")) if isinstance(res, dict) else str(res),
                    "status": str(res.get("status", "unclear")) if isinstance(res, dict) else "unclear",
                    "evidence": str(res.get("evidence", "")) if isinstance(res, dict) else "",
                }
                findings_by_index[int(idx)] = norm
                # persist ordered list in session_state
                ordered_findings = [findings_by_index.get(i, {"req_id": checklist_items[i].get("id",""), "status":"PENDING", "evidence":""}) for i in range(total_items)]
                st.session_state.findings = ordered_findings

                # render table in original checklist order using checklist text as Requirement column
                with findings_container:
                    rows = []
                    for i, f in enumerate(ordered_findings):
                        req_text = checklist_items[i].get("text", checklist_items[i].get("id", ""))
                        rows.append({
                            "Requirement": req_text,
                            "Status": (f.get("status") or "").upper(),
                            "Evidence": f.get("evidence", ""),
                        })
                    st.table(rows)

            st.session_state.analysis_done = True
            st.session_state.run_analysis = False

        if st.session_state.analysis_done:
            st.subheader("Results")
            st.table([{
                "Requirement": f.get("req_id", ""),
                "Status": (f.get("status") or "").upper(),
                "Evidence": f.get("evidence", ""),
            } for f in st.session_state.findings])

            # Human-facing summary counts
            findings = st.session_state.findings or []
            total_spec_items = len(st.session_state.checklist.get("submittals", []))
            present = sum(1 for f in findings if (f.get("status") or "").lower() == "present")
            missing = sum(1 for f in findings if (f.get("status") or "").lower() == "missing")
            not_applicable = sum(1 for f in findings if (f.get("status") or "").lower() == "not_applicable")
            unclear = sum(1 for f in findings if (f.get("status") or "").lower() == "unclear")
            applicable = total_spec_items - not_applicable

            st.subheader("Summary")
            pkg_type = st.session_state.classification.get("package_type", "Unknown")
            human_summary = (
                f"This appears to be a {pkg_type} submittal. The spec requires {total_spec_items} documents in total; "
                f"{applicable} apply to this package: {present} present, {missing} missing, {not_applicable} not applicable, {unclear} unclear."
            )
            st.write(human_summary)

            report = {
                "classification": st.session_state.classification,
                "checklist": st.session_state.checklist,
                "findings": findings,
                "summary": {
                    "total_spec_items": total_spec_items,
                    "applicable": applicable,
                    "present": present,
                    "missing": missing,
                    "not_applicable": not_applicable,
                    "unclear": unclear,
                    "human_summary": human_summary,
                },
            }

            st.subheader("Download Report")
            st.download_button(
                label="üì• Download JSON Report",
                data=json.dumps(report, indent=2),
                file_name="submittal_report.json",
                mime="application/json",
            )


if __name__ == "__main__":
    main()
